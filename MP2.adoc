= Machine Problems 2
Conrad Fernandez
:doctype: article
:imagesdir: ./images
:toc:

== Statement of originality

"I do hereby verify that this machine problem submission is my own work and contains my own original ideas, concepts, and designs. No portion of this report or code has been copied in whole or in part from another source, with the possible exception of properly referenced material". 

== Part 1
=== Code
[source,c]
----
include::tiled_matrix_multiplication/tiled_matrix_multiplication/kernel.cu[]
----
=== Analysis
image::tiled_matrix_output_example.png[]

Here is an example of the output from running the tiled matrix multiplication code. It runs each of these matrix sizes 5 times for a specified tile size: 100 x 100, 250 x 250, 500 x 500, 1000 x 1000, 1500 x 1500. The first column is matrix size, second coulmn is tile size, and the third column is the time taken to compute the matrix multiplication. The program is run for each tile size: 2, 5, 10, 25. This output format makes it easy to copy and paste to a csv file for easy plotting.


image::tiled_compute_time.png[]

The derived plot shows a trend in matrix multiplication compute time as a function of tile size, revealing a significant decrease in compute time as the tile size is increased from 2 to 5. This decline in time is especially marked for the largest matrix tested, 1500x1500, which shows a steep drop to compute times compared to much smaller matrices at a tile size of 5. Beyond a tile size of 10, the benefit of increasing tile size diminishes across all matrix sizes, with the plot curve flattening and  suggestingdiminishing returns in efficiency from larger tile sizes. Interestingly, the compute time for a 100x100 matrix remains flat across all tile sizes, suggesting that tiling overhead may negate any computational benefits for matrices of this small scale. 

== Part 2

=== Code
[source,c]
----
include::matrix_multiplication/matrix_multiplication/kernel.cu[]
----

=== Analysis
==== Memory Transfer Time vs. Matrix Size
image::data_transfer_time_vs_matrix_size.png[]

As can be seen in the plot, host to device memory transfer is faster than device to host memory transfer. This is expected as host-to-device transfers are often optimized for loading data into the GPU for processing. In contrast, device-to-host transfers are typically less frequent and may not be as heavily optimized, since the primary use case of GPUs is to perform computations rather than to act as sources of data.

==== GPU vs. CPU Time to Compute Matrix Multiplication
image::compute_time.png[]

As can be seen in the plot, calculating the matrix multiplication on the GPU is faster than the CPU when the matrix size is big enough to take advantage of the parallelization performance gains. Otherwise, there is too much data transfer overhead for it to be beneficial to use the GPU. This is expected as the GPU is designed to perform parallel computations, and matrix multiplication is a highly parallelizable operation.

====  GPU Block Width vs. Time to Compute Matrix Multiplication
image::block_time.png[]

As can be seen in the plot, the time to compute the matrix multiplication on the GPU decreases as the block size increases. This trend suggests that larger block sizes enable more efficient utilization of the GPU's resources, probably due to reduced overhead in managing smaller blocks and improved data locality. However, it seems there is diminishing performance gains as the block size passes 10.

==== a. How many times is each element of each input matrix loaded during the execution of the kernel?
Each element of each input matrix is loaded N * N times during the execution of the kernel because they are accessed in a nested loop that iterates N times for each element.

==== b. What is the floating-point computation to global memory access (CGMA) ratio in each thread? Consider multiply and addition as separate operations and ignore the global memory store at the end. Only count global memory loads towards your off-chip bandwidth.

**FLOPs**

For each iteration there is 1 multiplication and 1 addition totaling 2 FLOPs. Since the loop iterates N times, each thread performs 2N FLOPs.

**Global Memory Loads**

Each iteration of the loop loads 1 element from matrix a and 1 element from matrix b, totaling 2 loads. Over n iterations, each thread performs 2n global memory loads.

**CGMA Ratio**

Therefore, the CGMA ratio is FLOPs / Global Memory Loads = 2N / 2N = 1.