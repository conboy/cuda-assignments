= Machine Problems 2
Conrad Fernandez
:doctype: article
:imagesdir: ./images
:toc:

== Statement of originality

"I do hereby verify that this machine problem submission is my own work and contains my own original ideas, concepts, and designs. No portion of this report or code has been copied in whole or in part from another source, with the possible exception of properly referenced material". 

== Part 1
=== Code
[source,c]
----
include::tiled_matrix_multiplication/tiled_matrix_multiplication/kernel.cu[]
----
=== Analysis
image::tiled_matrix_output_example.png[]

Here is an example of the output from running the tiled matrix multiplication code. It runs each of these matrix sizes 5 times for a specified tile size: 100 x 100, 250 x 250, 500 x 500, 1000 x 1000, 1500 x 1500. The first column is matrix size, second coulmn is tile size, and the third column is the time taken to compute the matrix multiplication. The program is run for each tile size: 2, 5, 10, 25. This output format makes it easy to copy and paste to a csv file for easy plotting.


image::tiled_compute_time.png[]

The derived plot shows a trend in matrix multiplication compute time as a function of tile size, revealing a significant decrease in compute time as the tile size is increased from 2 to 5. This decline in time is especially marked for the largest matrix tested, 1500x1500, which shows a steep drop to compute times compared to much smaller matrices at a tile size of 5. Beyond a tile size of 10, the benefit of increasing tile size diminishes across all matrix sizes, with the plot curve flattening and  suggestingdiminishing returns in efficiency from larger tile sizes. Interestingly, the compute time for a 100x100 matrix remains flat across all tile sizes, suggesting that tiling overhead may negate any computational benefits for matrices of this small scale. 

image::normal_vs_tiled.png[]

To compare the tiled GPU matrix multiplication to the GPU matrix multiplication done in machine problems 1, I ran both implementations on the same sized input matrices to see their compute times. A tile size of 16 was used for the tiled matrix multiplication. To my surprise, the tiled matrix multiplication is slower than the normal matrix multiplication. This is likely due to the overhead of tiling. The normal matrix multiplication is faster because it is simpler and does not have the overhead of tiling.

Question 1: In your kernel implementation, how many threads can be simultaneously scheduled on your CUDA device, considering the number of streaming multiprocessors? 

image::sms.png[]

The number of threads that can be simultaneously scheduled on a CUDA device is determined by the number of streaming multiprocessors (SMs) and Maximum Threads per SM using the following equation: `Total Threads = Number of SMs x Maximum Threads per SM`

Therefore we get `Total Threads = 14 x 1024 = 14336 threads`

Question 2: Find the resource usage of your kernel, including the number of registers, shared memory size, number of blocks per streaming multiprocessor, and maximum total threads simultaneously scheduled/executing.

To find the resource usage of the kernel, I used the `cudaOccupancyMaxActiveBlocksPerMultiprocessor` and `cudaFuncGetAttributes` functions from the CUDA Runtime API.

The requested information was extracted from the kernel using the implemented `print_kernel_attributes` function in the code. The output is as follows:

image::resource_usage.png[]

To explain this output, each thread executing the kernel uses 42 registers which is high and contributes to poor resource usage. Each block of the kernel requires 5000 bytes of shared memory. There can only be 1 active block per SM which is a consequence of high resource usage (registers and shared memory). Lastly, the max total threads per SM is 625.

== Part 2

=== Code
[source,c]
----
include::revised_tiled_matrix_multiplication/revised_tiled_matrix_multiplication/kernel.cu[]
----

=== Analysis
==== Comparing GPU Tiled Matrix Multiplication to CPU Matrix Multiplication
image::revised_tiled_results.png[]

Here is the output from running the GPU revised tiled matrix multiplication against the CPU matrix multiplication implementation. It is clear to see that the tiled matrix multiplication implementation is much faster than the CPU's version thanks to the parallelism of the GPU.